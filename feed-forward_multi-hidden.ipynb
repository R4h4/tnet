{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, fetch_mldata\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "from cost import SquaredError, CrossEntropy\n",
    "\n",
    "\n",
    "def identity(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1-x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x, deriv=False, axis=1):\n",
    "    shift_x = x - np.max(x)\n",
    "    \n",
    "    try:\n",
    "        sm = np.exp(shift_x) / np.sum(np.exp(shift_x), axis=axis, keepdims=True)\n",
    "    except np.AxisError:\n",
    "        sm = np.exp(shift_x) / np.sum(np.exp(shift_x), axis=None, keepdims=True)\n",
    "        \n",
    "    if deriv: \n",
    "        return x * (1 - x) # https://datascience.stackexchange.com/questions/29735/how-to-apply-the-gradient-of-softmax-in-backprop\n",
    "    else:\n",
    "        return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43386458262986227"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.array([0.1, 0.1, 0.8])\n",
    "targets = np.array([0, 0, 1])\n",
    "cost = CrossEntropy()\n",
    "cost.error(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    # Class Attributes\n",
    "    train_error = list()\n",
    "    \n",
    "    def __init__(self, layers, cost_function):\n",
    "        assert isinstance(layers, list), \"Input needs to be a list of Layers\"\n",
    "        assert len(layers) > 1, \"Input needs to be a list of at least two Layers\"\n",
    "        self.layers = layers\n",
    "        self.x = np.zeros(1)\n",
    "        self.target = np.zeros(1)\n",
    "        self.current_state = np.zeros(1)\n",
    "        assert callable(cost_function), \"Chose a valid error function\"\n",
    "        self.cost_function = cost_function\n",
    "        self.l_error = list() # Error over time is saved here\n",
    "\n",
    "                \n",
    "    def load_data(self, x: np.ndarray, target: np.ndarray):\n",
    "        # Check if input and output have the same amount of cases \n",
    "        assert len(x) == len(target), f\"Input and target output contain a different number of cases ({len(x)} vs. {len(target)})\"\n",
    "        # Check if x and target are numeric numpy arrays\n",
    "        assert np.issubdtype(x.dtype, np.number) & np.issubdtype(target.dtype, np.number), \"Both input and target need to be numeric arrays\"\n",
    "        \n",
    "        self.x = x.copy()\n",
    "        self.target = target.copy()\n",
    "        \n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # First we infer the input size for each of the layer, except the first one\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                assert layer.input_size, \"The first layer need to be initialized with the parameter 'input_size'\"\n",
    "            else:\n",
    "                layer.input_size = self.layers[i-1].size\n",
    "\n",
    "        # Initialize the weights with random noise\n",
    "        np.random.RandomState(42)\n",
    "        sigma = 0.03\n",
    "        \n",
    "        # Then we initialize the weights by using the input size (+1 bias Unit) and amount of units\n",
    "        for layer in self.layers:\n",
    "            layer.weights = sigma * np.random.randn(layer.input_size + 1, layer.size)\n",
    "    \n",
    "    \n",
    "    def train(self, n_epochs: int, alpha=0.01):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Calculate forward\n",
    "            self.current_state = self.calc_output(self.x)\n",
    "            \n",
    "            error_epoch = self.cost_function(self.current_state, self.target)\n",
    "            logging.debug(f\"Error in epoch {epoch}: {error_epoch}\")\n",
    "            self.l_error.append(error_epoch)\n",
    "                    \n",
    "            # Calculate backwards\n",
    "            # Start with calculating the error/loss at each layer            \n",
    "            for i, layer in enumerate(reversed(self.layers)):\n",
    "                if i == 0:\n",
    "                    layer.error = np.subtract(self.current_state, self.target)\n",
    "                    # Start with calculating the error/loss at the output\n",
    "                else:\n",
    "                    layer.calc_error(prev_error=self.layers[len(self.layers) - i].error, prev_weights=self.layers[len(self.layers) - i].weights)\n",
    "            \n",
    "            # Then calculate the partial derivative and update the weights\n",
    "            for layer in self.layers:\n",
    "                layer.update_weights(alpha)\n",
    "                \n",
    "    def train_sgd(self, n_epochs: int, alpha=0.01):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "            for i_step, example in enumerate(self.x):\n",
    "                # Calculate forward\n",
    "                self.current_state = example\n",
    "                for layer in self.layers:\n",
    "                    layer.forward(self.current_state)\n",
    "                    self.current_state = layer.activations_out\n",
    "\n",
    "                mse_epoch = mse(self.current_state, self.target[i_step])\n",
    "                self.l_error.append(mse_epoch)\n",
    "                #if not (epoch % 10):\n",
    "                #    if mse_epoch > (min(self.l_error) * 1.1):\n",
    "                #        alpha = alpha/2\n",
    "                #        print(\"Devide alpha by 2\")\n",
    "\n",
    "                # Calculate backwards\n",
    "                # Start with calculating the error/loss at each layer            \n",
    "                for i, layer in enumerate(reversed(self.layers)):\n",
    "                    if i == 0:\n",
    "                        layer.error = np.subtract(self.current_state, self.target[i_step])\n",
    "                        # Start with calculating the error/loss at the output\n",
    "                    else:\n",
    "                        layer.calc_error(prev_error=self.layers[len(self.layers) - i].error, prev_weights=self.layers[len(self.layers) - i].weights)\n",
    "\n",
    "                # Then calculate the partial derivative and update the weights\n",
    "                for layer in self.layers:\n",
    "                    layer.update_weights(alpha)\n",
    "                \n",
    "    def plot_error(self):\n",
    "        plt.plot(range(len(self.l_error)), self.l_error)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def calc_output(self, _input):\n",
    "        # Calculate \n",
    "        current_state = _input\n",
    "        for layer in self.layers:\n",
    "            layer.forward(current_state)\n",
    "            current_state = layer.activations_out\n",
    "        return current_state\n",
    "    \n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, size: int, activation, input_size=False):\n",
    "        assert isinstance(size, int), \"The number of nodes needs to be of type int\"\n",
    "        self.size = size\n",
    "        assert callable(activation), \"Chose a valid activation function\"\n",
    "        self.activation = activation\n",
    "        self.activations_in = np.zeros(1)\n",
    "        self.activations_out = np.zeros(size)\n",
    "        self.error = np.zeros(size)\n",
    "        self.weights = np.zeros(size)\n",
    "        self.isfirst = False\n",
    "        self.input_size = input_size\n",
    "        if input_size:\n",
    "            self.isfirst = True\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def forward(self, activations_in):\n",
    "        # Save incoming activations for later backpropagation and add bias unit\n",
    "        if activations_in.ndim == 1:\n",
    "            ones_shape = 1\n",
    "        else:\n",
    "            ones_shape = (len(activations_in), 1) + activations_in.shape[2:]\n",
    "        self.activations_in = np.hstack((np.ones(shape=ones_shape), activations_in))\n",
    "        self.activations_out = self.activation(np.dot(self.activations_in, self.weights)) \n",
    "\n",
    "    def calc_error(self, prev_error, prev_weights):\n",
    "        self.error = np.dot(prev_error, prev_weights.T[:, 1:]) * self.activation(self.activations_out, deriv=True)\n",
    "    \n",
    "    def update_weights(self, alpha):\n",
    "        # The first layer does have one weight less due to the missing bias unit\n",
    "        # Calculate the partial derivatives for the Error in respect to each weight\n",
    "        if self.isfirst:\n",
    "            if self.activations_in.ndim == 1:\n",
    "                partial_derivative = self.activations_in[:, np.newaxis] * self.error[np.newaxis, :]\n",
    "                gradient = partial_derivative\n",
    "            else:\n",
    "                partial_derivative = self.activations_in[:, :1, np.newaxis] * self.error[: , np.newaxis, :]\n",
    "                gradient = np.average(partial_derivative, axis=0)\n",
    "        else:\n",
    "            if self.activations_in.ndim == 1:\n",
    "                partial_derivative = self.activations_in[:, np.newaxis] * self.error[np.newaxis, :]\n",
    "                gradient = partial_derivative\n",
    "            else:\n",
    "                partial_derivative = self.activations_in[:, :, np.newaxis] * self.error[: , np.newaxis, :]\n",
    "                gradient = np.average(partial_derivative, axis=0)\n",
    "        #print(f\"Weights before update: {self.weights}\")\n",
    "        self.weights += -alpha * gradient\n",
    "        #print(f\"Weights after update: {self.weights}\")\n",
    "        \n",
    "\n",
    "def batch_gd(weights, alpha, gradient):\n",
    "    return -alpha * gradient + weights\n",
    "    \n",
    "\n",
    "def stochastic_gd():\n",
    "    pass\n",
    "\n",
    "    \n",
    "def mini_batch_gd():\n",
    "    pass\n",
    "\n",
    "    \n",
    "class Activision(Layer):\n",
    "    \n",
    "    def forward(self, activations_in):\n",
    "        self.activations_out = self.activation(np.dot(activations_in, weights))\n",
    "        \n",
    "    def backward(self):\n",
    "        pass                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, batch_size, momentum=0.9):\n",
    "        pass\n",
    "        \n",
    "    def gradient_decent(self, weights, gradient, learning_rate=0.03):\n",
    "        return weights - learning_rate * gradient\n",
    "    \n",
    "    def momentum(self, gradient, rate=0.9):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level='ERROR')\n",
    "\n",
    "data = load_iris()\n",
    "x = data['data']\n",
    "target = data['target']\n",
    "\n",
    "# One-Hot-Encoding the output (since its categorical)\n",
    "n_categories = 3\n",
    "y = np.eye(n_categories)[target.astype(int)]\n",
    "\n",
    "#x, y = unison_shuffled_copies(x, y)\n",
    "\n",
    "nn = NeuralNetwork([Dense(6, sigmoid, input_size=4), Dense(3, softmax)], cost_function=SquaredError())\n",
    "nn.load_data(x, y)\n",
    "nn.init_weights()\n",
    "\n",
    "nn.train(50, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\google drive\\dev\\tnet\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "d:\\google drive\\dev\\tnet\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9d8946e3f64df1aa7da60927d934d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = fetch_mldata('MNIST original', data_home='./')\n",
    "\n",
    "x = data['data']\n",
    "target = data['target']\n",
    "\n",
    "# One-Hot-Encoding the output (since its categorical)\n",
    "n_categories = 10\n",
    "y = np.eye(n_categories)[target.astype(int)]\n",
    "\n",
    "nn = NeuralNetwork([Dense(100, sigmoid, input_size=784), Dense(n_categories, softmax)], cost_function=CrossEntropy())\n",
    "nn.load_data(x, y)\n",
    "nn.init_weights()\n",
    "\n",
    "nn.train(500, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nn.calc_output(x)\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "(prediction == target).sum() / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
