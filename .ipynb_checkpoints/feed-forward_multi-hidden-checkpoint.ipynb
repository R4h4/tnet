{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, fetch_mldata\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "from cost import SquaredError, CrossEntropy\n",
    "\n",
    "\n",
    "def identity(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1-x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x, deriv=False, axis=1):\n",
    "    shift_x = x - np.max(x)\n",
    "    \n",
    "    try:\n",
    "        sm = np.exp(shift_x) / np.sum(np.exp(shift_x), axis=axis, keepdims=True)\n",
    "    except np.AxisError:\n",
    "        sm = np.exp(shift_x) / np.sum(np.exp(shift_x), axis=None, keepdims=True)\n",
    "        \n",
    "    if deriv: \n",
    "        return x * (1 - x) # https://datascience.stackexchange.com/questions/29735/how-to-apply-the-gradient-of-softmax-in-backprop\n",
    "    else:\n",
    "        return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43386458262986227"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.array([0.1, 0.1, 0.8])\n",
    "targets = np.array([0, 0, 1])\n",
    "cost = CrossEntropy()\n",
    "cost.error(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    # Class Attributes\n",
    "    train_error = list()\n",
    "    \n",
    "    def __init__(self, layers, cost_function):\n",
    "        assert isinstance(layers, list), \"Input needs to be a list of Layers\"\n",
    "        assert len(layers) > 1, \"Input needs to be a list of at least two Layers\"\n",
    "        self.layers = layers\n",
    "        self.x = np.zeros(1)\n",
    "        self.target = np.zeros(1)\n",
    "        self.current_state = np.zeros(1)\n",
    "        assert callable(cost_function), \"Chose a valid error function\"\n",
    "        self.cost_function = cost_function\n",
    "        self.l_error = list() # Error over time is saved here\n",
    "\n",
    "                \n",
    "    def load_data(self, x: np.ndarray, target: np.ndarray):\n",
    "        # Check if input and output have the same amount of cases \n",
    "        assert len(x) == len(target), f\"Input and target output contain a different number of cases ({len(x)} vs. {len(target)})\"\n",
    "        # Check if x and target are numeric numpy arrays\n",
    "        assert np.issubdtype(x.dtype, np.number) & np.issubdtype(target.dtype, np.number), \"Both input and target need to be numeric arrays\"\n",
    "        \n",
    "        self.x = x.copy()\n",
    "        self.target = target.copy()\n",
    "        \n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # First we infer the input size for each of the layer, except the first one\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                assert layer.input_size, \"The first layer need to be initialized with the parameter 'input_size'\"\n",
    "            else:\n",
    "                layer.input_size = self.layers[i-1].size\n",
    "\n",
    "        # Initialize the weights with random noise\n",
    "        np.random.RandomState(42)\n",
    "        sigma = 0.03\n",
    "        \n",
    "        # Then we initialize the weights by using the input size (+1 bias Unit) and amount of units\n",
    "        for layer in self.layers:\n",
    "            layer.weights = sigma * np.random.randn(layer.input_size + 1, layer.size)\n",
    "    \n",
    "    \n",
    "    def train(self, n_epochs: int, alpha=0.01):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Calculate forward\n",
    "            self.current_state = self.calc_output(self.x)\n",
    "            \n",
    "            error_epoch = self.cost_function(self.current_state, self.target)\n",
    "            logging.debug(f\"Error in epoch {epoch}: {error_epoch}\")\n",
    "            self.l_error.append(error_epoch)\n",
    "                    \n",
    "            # Calculate backwards\n",
    "            # Start with calculating the error/loss at each layer            \n",
    "            for i, layer in enumerate(reversed(self.layers)):\n",
    "                if i == 0:\n",
    "                    layer.error = np.subtract(self.current_state, self.target)\n",
    "                    # Start with calculating the error/loss at the output\n",
    "                else:\n",
    "                    layer.calc_error(prev_error=self.layers[len(self.layers) - i].error, prev_weights=self.layers[len(self.layers) - i].weights)\n",
    "            \n",
    "            # Then calculate the partial derivative and update the weights\n",
    "            for layer in self.layers:\n",
    "                layer.update_weights(alpha)\n",
    "                \n",
    "    def train_sgd(self, n_epochs: int, alpha=0.01):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "            for i_step, example in enumerate(self.x):\n",
    "                # Calculate forward\n",
    "                self.current_state = example\n",
    "                for layer in self.layers:\n",
    "                    layer.forward(self.current_state)\n",
    "                    self.current_state = layer.activations_out\n",
    "\n",
    "                mse_epoch = mse(self.current_state, self.target[i_step])\n",
    "                self.l_error.append(mse_epoch)\n",
    "                #if not (epoch % 10):\n",
    "                #    if mse_epoch > (min(self.l_error) * 1.1):\n",
    "                #        alpha = alpha/2\n",
    "                #        print(\"Devide alpha by 2\")\n",
    "\n",
    "                # Calculate backwards\n",
    "                # Start with calculating the error/loss at each layer            \n",
    "                for i, layer in enumerate(reversed(self.layers)):\n",
    "                    if i == 0:\n",
    "                        layer.error = np.subtract(self.current_state, self.target[i_step])\n",
    "                        # Start with calculating the error/loss at the output\n",
    "                    else:\n",
    "                        layer.calc_error(prev_error=self.layers[len(self.layers) - i].error, prev_weights=self.layers[len(self.layers) - i].weights)\n",
    "\n",
    "                # Then calculate the partial derivative and update the weights\n",
    "                for layer in self.layers:\n",
    "                    layer.update_weights(alpha)\n",
    "                \n",
    "    def plot_error(self):\n",
    "        plt.plot(range(len(self.l_error)), self.l_error)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def calc_output(self, _input):\n",
    "        # Calculate \n",
    "        current_state = _input\n",
    "        for layer in self.layers:\n",
    "            layer.forward(current_state)\n",
    "            current_state = layer.activations_out\n",
    "        return current_state\n",
    "    \n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, size: int, activation, input_size=False):\n",
    "        assert isinstance(size, int), \"The number of nodes needs to be of type int\"\n",
    "        self.size = size\n",
    "        assert callable(activation), \"Chose a valid activation function\"\n",
    "        self.activation = activation\n",
    "        self.activations_in = np.zeros(1)\n",
    "        self.activations_out = np.zeros(size)\n",
    "        self.error = np.zeros(size)\n",
    "        self.weights = np.zeros(size)\n",
    "        self.isfirst = False\n",
    "        self.input_size = input_size\n",
    "        if input_size:\n",
    "            self.isfirst = True\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def forward(self, activations_in):\n",
    "        # Save incoming activations for later backpropagation and add bias unit\n",
    "        if activations_in.ndim == 1:\n",
    "            ones_shape = 1\n",
    "        else:\n",
    "            ones_shape = (len(activations_in), 1) + activations_in.shape[2:]\n",
    "        self.activations_in = np.hstack((np.ones(shape=ones_shape), activations_in))\n",
    "        self.activations_out = self.activation(np.dot(self.activations_in, self.weights)) \n",
    "\n",
    "    def calc_error(self, prev_error, prev_weights):\n",
    "        self.error = np.dot(prev_error, prev_weights.T[:, 1:]) * self.activation(self.activations_out, deriv=True)\n",
    "    \n",
    "    def update_weights(self, alpha):\n",
    "        # The first layer does have one weight less due to the missing bias unit\n",
    "        # Calculate the partial derivatives for the Error in respect to each weight\n",
    "        if self.isfirst:\n",
    "            if self.activations_in.ndim == 1:\n",
    "                partial_derivative = self.activations_in[:, np.newaxis] * self.error[np.newaxis, :]\n",
    "                gradient = partial_derivative\n",
    "            else:\n",
    "                partial_derivative = self.activations_in[:, :1, np.newaxis] * self.error[: , np.newaxis, :]\n",
    "                gradient = np.average(partial_derivative, axis=0)\n",
    "        else:\n",
    "            if self.activations_in.ndim == 1:\n",
    "                partial_derivative = self.activations_in[:, np.newaxis] * self.error[np.newaxis, :]\n",
    "                gradient = partial_derivative\n",
    "            else:\n",
    "                partial_derivative = self.activations_in[:, :, np.newaxis] * self.error[: , np.newaxis, :]\n",
    "                gradient = np.average(partial_derivative, axis=0)\n",
    "        #print(f\"Weights before update: {self.weights}\")\n",
    "        self.weights += -alpha * gradient\n",
    "        #print(f\"Weights after update: {self.weights}\")\n",
    "        \n",
    "\n",
    "def batch_gd(weights, alpha, gradient):\n",
    "    return -alpha * gradient + weights\n",
    "    \n",
    "\n",
    "def stochastic_gd():\n",
    "    pass\n",
    "\n",
    "    \n",
    "def mini_batch_gd():\n",
    "    pass\n",
    "\n",
    "    \n",
    "class Activision(Layer):\n",
    "    \n",
    "    def forward(self, activations_in):\n",
    "        self.activations_out = self.activation(np.dot(activations_in, weights))\n",
    "        \n",
    "    def backward(self):\n",
    "        pass                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, batch_size, momentum=0.9):\n",
    "        pass\n",
    "        \n",
    "    def gradient_decent(self, weights, gradient, learning_rate=0.03):\n",
    "        return weights - learning_rate * gradient\n",
    "    \n",
    "    def momentum(self, gradient, rate=0.9):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level='ERROR')\n",
    "\n",
    "data = load_iris()\n",
    "x = data['data']\n",
    "target = data['target']\n",
    "\n",
    "# One-Hot-Encoding the output (since its categorical)\n",
    "n_categories = 3\n",
    "y = np.eye(n_categories)[target.astype(int)]\n",
    "\n",
    "#x, y = unison_shuffled_copies(x, y)\n",
    "\n",
    "nn = NeuralNetwork([Dense(6, sigmoid, input_size=4), Dense(3, softmax)], cost_function=SquaredError())\n",
    "nn.load_data(x, y)\n",
    "nn.init_weights()\n",
    "\n",
    "nn.train(50, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\google drive\\dev\\tnet\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "d:\\google drive\\dev\\tnet\\venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9d8946e3f64df1aa7da60927d934d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = fetch_mldata('MNIST original', data_home='./')\n",
    "\n",
    "x = data['data']\n",
    "target = data['target']\n",
    "\n",
    "# One-Hot-Encoding the output (since its categorical)\n",
    "n_categories = 10\n",
    "y = np.eye(n_categories)[target.astype(int)]\n",
    "\n",
    "nn = NeuralNetwork([Dense(100, sigmoid, input_size=784), Dense(n_categories, softmax)], cost_function=CrossEntropy())\n",
    "nn.load_data(x, y)\n",
    "nn.init_weights()\n",
    "\n",
    "nn.train(500, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.plot_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = nn.calc_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8005571428571429"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prediction == target).sum() / len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build and train your neural network:\n",
    "It starts with initializing a Neural network. A neural network consaists of a list of layers and a cost function. \n",
    "Each layer takes the following parameters:\n",
    "\n",
    "   `size`: Int, The number of nodes in this layer. For the last layer, this has to be the output size.\n",
    "   `activation`: Function, The activation function for this layer.\n",
    "   `input_size`: Int, Required on the first first layer, needs to be equal to the size of the input vector for each set.\n",
    "\n",
    "After the nn is initialized, we can load the data with `.load(input, target)`\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "`nn = NeuralNetwork([Dense(100, sigmoid, input_size=784), Dense(10, softmax)], cost_function=CrossEntropy())`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
