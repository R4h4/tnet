{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "from cost import SquaredError, CrossEntropy\n",
    "\n",
    "\n",
    "def identity(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1-x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x, deriv=False, axis=1):\n",
    "    shift_x = x - np.max(x)\n",
    "    \n",
    "    try:\n",
    "        sm = np.exp(shift_x) / np.sum(np.exp(shift_x), axis=axis, keepdims=True)\n",
    "    except np.AxisError:\n",
    "        sm = np.exp(shift_x) / np.sum(np.exp(shift_x), axis=None, keepdims=True)\n",
    "        \n",
    "    if deriv: \n",
    "        return x * (1 - x) # https://datascience.stackexchange.com/questions/29735/how-to-apply-the-gradient-of-softmax-in-backprop\n",
    "    else:\n",
    "        return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43386458262986227"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.array([0.1, 0.1, 0.8])\n",
    "targets = np.array([0, 0, 1])\n",
    "cost = CrossEntropy()\n",
    "cost.error(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    # Class Attributes\n",
    "    train_error = list()\n",
    "    \n",
    "    def __init__(self, layers, cost_function):\n",
    "        assert isinstance(layers, list), \"Input needs to be a list of Layers\"\n",
    "        assert len(layers) > 1, \"Input needs to be a list of at least two Layers\"\n",
    "        self.layers = layers\n",
    "        self.x = np.zeros(1)\n",
    "        self.target = np.zeros(1)\n",
    "        self.current_state = np.zeros(1)\n",
    "        assert callable(cost_function), \"Chose a valid error function\"\n",
    "        self.cost_function = cost_function\n",
    "        self.l_error = list() # Error over time is saved here\n",
    "\n",
    "                \n",
    "    def load_data(self, x: np.ndarray, target: np.ndarray):\n",
    "        # Check if input and output have the same amount of cases \n",
    "        assert len(x) == len(target), f\"Input and target output contain a different number of cases ({len(x)} vs. {len(target)})\"\n",
    "        # Check if x and target are numeric numpy arrays\n",
    "        assert np.issubdtype(x.dtype, np.number) & np.issubdtype(target.dtype, np.number), \"Both input and target need to be numeric arrays\"\n",
    "        \n",
    "        self.x = x.copy()\n",
    "        self.target = target.copy()\n",
    "        \n",
    "    \n",
    "    def init_weights(self):\n",
    "        # First we infer the input size for each of the layer, except the first one\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i == 0:\n",
    "                assert layer.input_size, \"The first layer need to be initialized with the parameter 'input_size'\"\n",
    "            else:\n",
    "                layer.input_size = self.layers[i-1].size\n",
    "\n",
    "        # Initialize the weights with random noise\n",
    "        np.random.RandomState(42)\n",
    "        sigma = 0.03\n",
    "        \n",
    "        # Then we initialize the weights by using the input size (+1 bias Unit) and amount of units\n",
    "        for layer in self.layers:\n",
    "            layer.weights = sigma * np.random.randn(layer.input_size + 1, layer.size)\n",
    "    \n",
    "    \n",
    "    def train(self, n_epochs: int, alpha=0.01):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Calculate forward\n",
    "            self.current_state = self.calc_output(self.x)\n",
    "            \n",
    "            error_epoch = self.cost_function(self.current_state, self.target)\n",
    "            logging.debug(f\"Error in epoch {epoch}: {error_epoch}\")\n",
    "            self.l_error.append(error_epoch)\n",
    "                    \n",
    "            # Calculate backwards\n",
    "            # Start with calculating the error/loss at each layer            \n",
    "            for i, layer in enumerate(reversed(self.layers)):\n",
    "                if i == 0:\n",
    "                    layer.error = np.subtract(self.current_state, self.target)\n",
    "                    # Start with calculating the error/loss at the output\n",
    "                else:\n",
    "                    layer.calc_error(prev_error=self.layers[len(self.layers) - i].error, prev_weights=self.layers[len(self.layers) - i].weights)\n",
    "            \n",
    "            # Then calculate the partial derivative and update the weights\n",
    "            for layer in self.layers:\n",
    "                layer.update_weights(alpha)\n",
    "                \n",
    "    def train_sgd(self, n_epochs: int, alpha=0.01):\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Shuffle the examples in the beginning of each epoch:\n",
    "            self.x, self.target = unison_shuffled_copies(self.x, self.target)\n",
    "            for i_step, example in enumerate(self.x):\n",
    "                # Calculate forward\n",
    "                self.current_state = example\n",
    "                for layer in self.layers:\n",
    "                    layer.forward(self.current_state)\n",
    "                    self.current_state = layer.activations_out\n",
    "\n",
    "                mse_epoch = mse(self.current_state, self.target[i_step])\n",
    "                self.l_error.append(mse_epoch)\n",
    "                #if not (epoch % 10):\n",
    "                #    if mse_epoch > (min(self.l_error) * 1.1):\n",
    "                #        alpha = alpha/2\n",
    "                #        print(\"Devide alpha by 2\")\n",
    "\n",
    "                # Calculate backwards\n",
    "                # Start with calculating the error/loss at each layer            \n",
    "                for i, layer in enumerate(reversed(self.layers)):\n",
    "                    if i == 0:\n",
    "                        layer.error = np.subtract(self.current_state, self.target[i_step])\n",
    "                        # Start with calculating the error/loss at the output\n",
    "                    else:\n",
    "                        layer.calc_error(prev_error=self.layers[len(self.layers) - i].error, prev_weights=self.layers[len(self.layers) - i].weights)\n",
    "\n",
    "                # Then calculate the partial derivative and update the weights\n",
    "                for layer in self.layers:\n",
    "                    layer.update_weights(alpha)\n",
    "                \n",
    "    def plot_error(self):\n",
    "        plt.plot(range(len(self.l_error)), self.l_error)\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def calc_output(self, _input):\n",
    "        # Calculate \n",
    "        current_state = _input\n",
    "        for layer in self.layers:\n",
    "            layer.forward(current_state)\n",
    "            current_state = layer.activations_out\n",
    "        return current_state\n",
    "    \n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, size: int, activation, input_size=False):\n",
    "        assert isinstance(size, int), \"The number of nodes needs to be of type int\"\n",
    "        self.size = size\n",
    "        assert callable(activation), \"Chose a valid activation function\"\n",
    "        self.activation = activation\n",
    "        self.activations_in = np.zeros(1)\n",
    "        self.activations_out = np.zeros(size)\n",
    "        self.error = np.zeros(size)\n",
    "        self.weights = np.zeros(size)\n",
    "        self.isfirst = False\n",
    "        self.input_size = input_size\n",
    "        if input_size:\n",
    "            self.isfirst = True\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def forward(self, activations_in):\n",
    "        # Save incoming activations for later backpropagation and add bias unit\n",
    "        if activations_in.ndim == 1:\n",
    "            ones_shape = 1\n",
    "        else:\n",
    "            ones_shape = (len(activations_in), 1) + activations_in.shape[2:]\n",
    "        self.activations_in = np.hstack((np.ones(shape=ones_shape), activations_in))\n",
    "        self.activations_out = self.activation(np.dot(self.activations_in, self.weights)) \n",
    "\n",
    "    def calc_error(self, prev_error, prev_weights):\n",
    "        self.error = np.dot(prev_error, prev_weights.T[:, 1:]) * self.activation(self.activations_out, deriv=True)\n",
    "    \n",
    "    def update_weights(self, alpha):\n",
    "        # The first layer does have one weight less due to the missing bias unit\n",
    "        # Calculate the partial derivatives for the Error in respect to each weight\n",
    "        if self.isfirst:\n",
    "            if self.activations_in.ndim == 1:\n",
    "                partial_derivative = self.activations_in[:, np.newaxis] * self.error[np.newaxis, :]\n",
    "                gradient = partial_derivative\n",
    "            else:\n",
    "                partial_derivative = self.activations_in[:, :1, np.newaxis] * self.error[: , np.newaxis, :]\n",
    "                gradient = np.average(partial_derivative, axis=0)\n",
    "        else:\n",
    "            if self.activations_in.ndim == 1:\n",
    "                partial_derivative = self.activations_in[:, np.newaxis] * self.error[np.newaxis, :]\n",
    "                gradient = partial_derivative\n",
    "            else:\n",
    "                partial_derivative = self.activations_in[:, :1, np.newaxis] * self.error[: , np.newaxis, :]\n",
    "                gradient = np.average(partial_derivative, axis=0)\n",
    "        print(f\"Weights before update: {self.weights}\")\n",
    "        self.weights += -alpha * gradient\n",
    "        print(f\"Weights after update: {self.weights}\")\n",
    "        \n",
    "\n",
    "def batch_gd(weights, alpha, gradient):\n",
    "    return -alpha * gradient + weights\n",
    "    \n",
    "\n",
    "def stochastic_gd():\n",
    "    pass\n",
    "\n",
    "    \n",
    "def mini_batch_gd():\n",
    "    pass\n",
    "\n",
    "    \n",
    "class Activision(Layer):\n",
    "    \n",
    "    def forward(self, activations_in):\n",
    "        self.activations_out = self.activation(np.dot(activations_in, weights))\n",
    "        \n",
    "    def backward(self):\n",
    "        pass                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, batch_size, momentum=0.9):\n",
    "        pass\n",
    "        \n",
    "    def gradient_decent(self, weights, gradient, learning_rate=0.03):\n",
    "        return weights - learning_rate * gradient\n",
    "    \n",
    "    def momentum(self, gradient, rate=0.9):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742c51d88c75492fbb55efa91a898113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Error in epoch 0: 1.0990783740452996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before update: [[-0.02772367 -0.00646534  0.04736221 -0.00926015 -0.01555423 -0.01192202]\n",
      " [ 0.03469633 -0.00716856  0.03361314 -0.05712703 -0.03408224  0.01645625]\n",
      " [-0.01760202 -0.00697929  0.01390461 -0.03583611 -0.03641061 -0.00173555]\n",
      " [-0.00779518 -0.02259188 -0.00956332 -0.03349424  0.00338069 -0.04870029]\n",
      " [-0.0401065  -0.01818815  0.03282993 -0.00487407  0.00055935  0.01231794]]\n",
      "Weights after update: [[-0.02772183 -0.00646467  0.04735751 -0.0092579  -0.01556396 -0.01192069]\n",
      " [ 0.03469818 -0.00716788  0.03360843 -0.05712479 -0.03409197  0.01645758]\n",
      " [-0.01760018 -0.00697861  0.0138999  -0.03583387 -0.03642034 -0.00173422]\n",
      " [-0.00779333 -0.0225912  -0.00956802 -0.033492    0.00337097 -0.04869896]\n",
      " [-0.04010466 -0.01818747  0.03282522 -0.00487182  0.00054962  0.01231928]]\n",
      "Weights before update: [[-0.03020193  0.0417787   0.02946888]\n",
      " [ 0.01516391  0.03478377 -0.00293758]\n",
      " [ 0.02538204  0.01346112  0.02126862]\n",
      " [-0.00395031 -0.00966606  0.03414918]\n",
      " [ 0.03866099 -0.01123609  0.03023067]\n",
      " [-0.03645153  0.01183434  0.0423224 ]\n",
      " [ 0.01258739 -0.02716506  0.00553556]]\n",
      "Weights after update: [[-0.02969383  0.04173172  0.02900776]\n",
      " [ 0.01567201  0.03473679 -0.0033987 ]\n",
      " [ 0.02589014  0.01341414  0.0208075 ]\n",
      " [-0.00344221 -0.00971303  0.03368806]\n",
      " [ 0.03916909 -0.01128307  0.02976955]\n",
      " [-0.03594343  0.01178736  0.04186128]\n",
      " [ 0.01309549 -0.02721204  0.00507444]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Error in epoch 1: 1.0990001889779002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before update: [[-0.02772183 -0.00646467  0.04735751 -0.0092579  -0.01556396 -0.01192069]\n",
      " [ 0.03469818 -0.00716788  0.03360843 -0.05712479 -0.03409197  0.01645758]\n",
      " [-0.01760018 -0.00697861  0.0138999  -0.03583387 -0.03642034 -0.00173422]\n",
      " [-0.00779333 -0.0225912  -0.00956802 -0.033492    0.00337097 -0.04869896]\n",
      " [-0.04010466 -0.01818747  0.03282522 -0.00487182  0.00054962  0.01231928]]\n",
      "Weights after update: [[-0.02771995 -0.00646389  0.04735309 -0.00925553 -0.01557322 -0.01191928]\n",
      " [ 0.03470006 -0.0071671   0.03360401 -0.05712242 -0.03410123  0.01645899]\n",
      " [-0.0175983  -0.00697783  0.01389548 -0.0358315  -0.0364296  -0.0017328 ]\n",
      " [-0.00779145 -0.02259042 -0.00957244 -0.03348963  0.00336171 -0.04869755]\n",
      " [-0.04010278 -0.01818669  0.0328208  -0.00486945  0.00054037  0.01232069]]\n",
      "Weights before update: [[-0.02969383  0.04173172  0.02900776]\n",
      " [ 0.01567201  0.03473679 -0.0033987 ]\n",
      " [ 0.02589014  0.01341414  0.0208075 ]\n",
      " [-0.00344221 -0.00971303  0.03368806]\n",
      " [ 0.03916909 -0.01128307  0.02976955]\n",
      " [-0.03594343  0.01178736  0.04186128]\n",
      " [ 0.01309549 -0.02721204  0.00507444]]\n",
      "Weights after update: [[-0.02920464  0.04168594  0.02856435]\n",
      " [ 0.0161612   0.03469102 -0.00384211]\n",
      " [ 0.02637933  0.01336837  0.02036409]\n",
      " [-0.00295302 -0.00975881  0.03324465]\n",
      " [ 0.03965828 -0.01132885  0.02932614]\n",
      " [-0.03545424  0.01174158  0.04141786]\n",
      " [ 0.01358468 -0.02725781  0.00463103]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level='DEBUG')\n",
    "\n",
    "data = load_iris()\n",
    "x = data['data']\n",
    "target = data['target']\n",
    "\n",
    "# One-Hot-Encoding the output (since its categorical)\n",
    "n_categories = 3\n",
    "y = np.eye(n_categories)[target.astype(int)]\n",
    "\n",
    "#x, y = unison_shuffled_copies(x, y)\n",
    "\n",
    "nn = NeuralNetwork([Dense(6, sigmoid, input_size=4), Dense(3, softmax)], cost_function=CrossEntropy())\n",
    "nn.load_data(x, y)\n",
    "nn.init_weights()\n",
    "\n",
    "nn.train(2, alpha=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n",
      "DEBUG:matplotlib.axes._base:update_title_pos\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.plot_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33078614, 0.33495947, 0.33425439],\n",
       "       [0.33089411, 0.33492147, 0.33418442],\n",
       "       [0.33085633, 0.33491245, 0.33423123],\n",
       "       [0.33092847, 0.33484929, 0.33422224],\n",
       "       [0.33077844, 0.33494359, 0.33427797],\n",
       "       [0.33075835, 0.33489391, 0.33434774],\n",
       "       [0.33085602, 0.33484916, 0.33429482],\n",
       "       [0.33083589, 0.33491912, 0.33424499],\n",
       "       [0.33096133, 0.33483656, 0.33420211],\n",
       "       [0.33089592, 0.33492525, 0.33417883],\n",
       "       [0.33074347, 0.33498872, 0.33426781],\n",
       "       [0.33087796, 0.33486287, 0.33425917],\n",
       "       [0.33090121, 0.33493028, 0.33416851],\n",
       "       [0.33088374, 0.3349145 , 0.33420176],\n",
       "       [0.33058496, 0.33512597, 0.33428907],\n",
       "       [0.33059688, 0.33499343, 0.33440969],\n",
       "       [0.33066972, 0.33498456, 0.33434572],\n",
       "       [0.33078896, 0.33493387, 0.33427716],\n",
       "       [0.3307433 , 0.33496877, 0.33428793],\n",
       "       [0.33075842, 0.3349138 , 0.33432778],\n",
       "       [0.33084066, 0.33494073, 0.33421861],\n",
       "       [0.33077882, 0.33488734, 0.33433384],\n",
       "       [0.330729  , 0.33496735, 0.33430364],\n",
       "       [0.33089657, 0.33481283, 0.33429061],\n",
       "       [0.33094483, 0.3347947 , 0.33426047],\n",
       "       [0.33092876, 0.3348928 , 0.33417844],\n",
       "       [0.33086378, 0.33484525, 0.33429098],\n",
       "       [0.33079846, 0.33495352, 0.33424803],\n",
       "       [0.33079384, 0.33497534, 0.33423083],\n",
       "       [0.33092319, 0.33484427, 0.33423254],\n",
       "       [0.33093095, 0.33486014, 0.33420891],\n",
       "       [0.33080184, 0.33493495, 0.33426321],\n",
       "       [0.33069024, 0.33498426, 0.3343255 ],\n",
       "       [0.33062382, 0.33503226, 0.33434392],\n",
       "       [0.33089873, 0.33489965, 0.33420161],\n",
       "       [0.3308044 , 0.33498548, 0.33421012],\n",
       "       [0.33072433, 0.33504909, 0.33422658],\n",
       "       [0.3307855 , 0.33495243, 0.33426207],\n",
       "       [0.33092131, 0.33486023, 0.33421846],\n",
       "       [0.33082598, 0.33493588, 0.33423814],\n",
       "       [0.33077664, 0.33493982, 0.33428355],\n",
       "       [0.33103822, 0.33484488, 0.3341169 ],\n",
       "       [0.33088596, 0.33486207, 0.33425196],\n",
       "       [0.33085179, 0.33479499, 0.33435322],\n",
       "       [0.33085011, 0.33479747, 0.33435242],\n",
       "       [0.33090684, 0.3348791 , 0.33421406],\n",
       "       [0.3307778 , 0.3349167 , 0.3343055 ],\n",
       "       [0.3308885 , 0.33487293, 0.33423857],\n",
       "       [0.33075338, 0.33497199, 0.33427463],\n",
       "       [0.33083127, 0.33494094, 0.33422779],\n",
       "       [0.33141635, 0.33421992, 0.33436373],\n",
       "       [0.33143619, 0.33413945, 0.33442436],\n",
       "       [0.33149204, 0.33413148, 0.33437648],\n",
       "       [0.33157116, 0.33414338, 0.33428546],\n",
       "       [0.33151967, 0.33412942, 0.33435091],\n",
       "       [0.33157372, 0.33406858, 0.3343577 ],\n",
       "       [0.3314762 , 0.33405303, 0.33447077],\n",
       "       [0.33144841, 0.33427906, 0.33427253],\n",
       "       [0.33148599, 0.33419797, 0.33431604],\n",
       "       [0.33151044, 0.33409439, 0.33439517],\n",
       "       [0.33155477, 0.33424626, 0.33419897],\n",
       "       [0.33145582, 0.3341216 , 0.33442259],\n",
       "       [0.33152972, 0.33430279, 0.33416749],\n",
       "       [0.33156259, 0.33406601, 0.33437139],\n",
       "       [0.33136413, 0.33425725, 0.33437862],\n",
       "       [0.33139801, 0.33423674, 0.33436526],\n",
       "       [0.33155373, 0.33400312, 0.33444315],\n",
       "       [0.33148325, 0.33425153, 0.33426522],\n",
       "       [0.33163537, 0.33409553, 0.33426911],\n",
       "       [0.33149722, 0.3342359 , 0.33426689],\n",
       "       [0.33156301, 0.33391161, 0.33452538],\n",
       "       [0.33142068, 0.33424933, 0.33432999],\n",
       "       [0.33166125, 0.33402461, 0.33431414],\n",
       "       [0.33157493, 0.33411574, 0.33430933],\n",
       "       [0.33143934, 0.33423254, 0.33432812],\n",
       "       [0.33142612, 0.33421898, 0.3343549 ],\n",
       "       [0.33153078, 0.33415969, 0.33430953],\n",
       "       [0.3315585 , 0.33402363, 0.33441787],\n",
       "       [0.33153073, 0.33406926, 0.33440001],\n",
       "       [0.33137668, 0.33437015, 0.33425317],\n",
       "       [0.33150279, 0.3342408 , 0.33425641],\n",
       "       [0.33147755, 0.33428902, 0.33423343],\n",
       "       [0.33144393, 0.3342461 , 0.33430997],\n",
       "       [0.33170402, 0.33390561, 0.33439038],\n",
       "       [0.3315741 , 0.33396951, 0.3344564 ],\n",
       "       [0.33144431, 0.3340491 , 0.33450659],\n",
       "       [0.33146792, 0.33414333, 0.33438874],\n",
       "       [0.33157921, 0.33418691, 0.33423389],\n",
       "       [0.33145841, 0.33414471, 0.33439689],\n",
       "       [0.3315354 , 0.33414546, 0.33431914],\n",
       "       [0.33160464, 0.33408098, 0.33431438],\n",
       "       [0.33152229, 0.33408975, 0.33438795],\n",
       "       [0.33148421, 0.33422235, 0.33429343],\n",
       "       [0.33145618, 0.33429488, 0.33424894],\n",
       "       [0.33153441, 0.33411889, 0.3343467 ],\n",
       "       [0.33146788, 0.3341642 , 0.33436791],\n",
       "       [0.33148851, 0.33413777, 0.33437373],\n",
       "       [0.33145985, 0.334199  , 0.33434115],\n",
       "       [0.33134586, 0.33435651, 0.33429763],\n",
       "       [0.33148392, 0.33415945, 0.33435663],\n",
       "       [0.33179266, 0.333532  , 0.33467534],\n",
       "       [0.33173282, 0.3337961 , 0.33447108],\n",
       "       [0.33172979, 0.33378582, 0.3344844 ],\n",
       "       [0.33175519, 0.33379417, 0.33445063],\n",
       "       [0.33177251, 0.33368271, 0.33454478],\n",
       "       [0.33183444, 0.33371117, 0.33445439],\n",
       "       [0.33171987, 0.33382929, 0.33445084],\n",
       "       [0.33180839, 0.33380324, 0.33438837],\n",
       "       [0.33183041, 0.33381157, 0.33435802],\n",
       "       [0.33166783, 0.33366355, 0.33466862],\n",
       "       [0.33157414, 0.33389375, 0.33453211],\n",
       "       [0.33171601, 0.33385153, 0.33443247],\n",
       "       [0.33167127, 0.33382609, 0.33450264],\n",
       "       [0.33175917, 0.3337745 , 0.33446633],\n",
       "       [0.33172869, 0.33367105, 0.33460025],\n",
       "       [0.33163757, 0.33375611, 0.33460632],\n",
       "       [0.33169414, 0.33385149, 0.33445437],\n",
       "       [0.33170581, 0.33368915, 0.33460504],\n",
       "       [0.33196852, 0.3336053 , 0.33442617],\n",
       "       [0.33176837, 0.33394828, 0.33428336],\n",
       "       [0.33167546, 0.33374944, 0.3345751 ],\n",
       "       [0.33169319, 0.33378366, 0.33452315],\n",
       "       [0.33187951, 0.33372818, 0.33439231],\n",
       "       [0.33163373, 0.33395074, 0.33441553],\n",
       "       [0.33167289, 0.33376736, 0.33455975],\n",
       "       [0.33169777, 0.3338577 , 0.33444452],\n",
       "       [0.33160369, 0.3339577 , 0.33443861],\n",
       "       [0.33160067, 0.33392037, 0.33447896],\n",
       "       [0.33177101, 0.33373417, 0.33449482],\n",
       "       [0.33168321, 0.33395134, 0.33436544],\n",
       "       [0.33177363, 0.33383898, 0.33438739],\n",
       "       [0.3316121 , 0.3338404 , 0.3345475 ],\n",
       "       [0.33177375, 0.33370897, 0.33451728],\n",
       "       [0.33165249, 0.33398239, 0.33436513],\n",
       "       [0.3318186 , 0.33385845, 0.33432294],\n",
       "       [0.33171715, 0.33379066, 0.33449218],\n",
       "       [0.33168222, 0.3336486 , 0.33466918],\n",
       "       [0.33168659, 0.3338358 , 0.33447761],\n",
       "       [0.33158848, 0.33392626, 0.33448526],\n",
       "       [0.33162058, 0.33386656, 0.33451287],\n",
       "       [0.33169444, 0.33371229, 0.33459327],\n",
       "       [0.33155884, 0.33388401, 0.33455715],\n",
       "       [0.33173282, 0.3337961 , 0.33447108],\n",
       "       [0.33173074, 0.33368747, 0.33458178],\n",
       "       [0.33168385, 0.33366681, 0.33464934],\n",
       "       [0.33161984, 0.33382682, 0.33455334],\n",
       "       [0.33169474, 0.33390061, 0.33440466],\n",
       "       [0.3316323 , 0.33386896, 0.33449875],\n",
       "       [0.3316449 , 0.33370217, 0.33465293],\n",
       "       [0.33166611, 0.33384145, 0.33449244]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.calc_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
